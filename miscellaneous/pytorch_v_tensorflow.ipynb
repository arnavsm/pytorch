{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate the difference between static and dynamic computational graphs using PyTorch (for dynamic graphs) and TensorFlow 1.x (for static graphs). We'll implement a simple example of a dynamic sentence encoder that adapts to the length of the input sentence.\n",
    "\n",
    "Let's start with the dynamic graph approach using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation (Dynamic Graph)\n",
    "class DynamicSentenceEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embedded = self.embedding(sentence)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        return hidden.squeeze(0)\n",
    "\n",
    "# Example usage of PyTorch model\n",
    "vocab_size, embedding_dim, hidden_dim = 1000, 50, 100\n",
    "pytorch_model = DynamicSentenceEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Different length sentences\n",
    "short_sentence = torch.LongTensor([[1, 2, 3, 4, 5]])\n",
    "long_sentence = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "\n",
    "# Process sentences of different lengths\n",
    "short_encoding = pytorch_model(short_sentence)\n",
    "long_encoding = pytorch_model(long_sentence)\n",
    "\n",
    "print(\"PyTorch (Dynamic) - Short sentence encoding shape:\", short_encoding.shape)\n",
    "print(\"PyTorch (Dynamic) - Long sentence encoding shape:\", long_encoding.shape)\n",
    "\n",
    "# TensorFlow 1.x implementation (Static Graph)\n",
    "class StaticSentenceEncoder:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, max_seq_length):\n",
    "        self.inputs = tf.placeholder(tf.int32, shape=[None, max_seq_length])\n",
    "        self.seq_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, embedding_dim])\n",
    "        embedded = tf.nn.embedding_lookup(embedding, self.inputs)\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.GRUCell(hidden_dim)\n",
    "        _, self.state = tf.nn.dynamic_rnn(cell, embedded, sequence_length=self.seq_lengths, dtype=tf.float32)\n",
    "\n",
    "# Example usage of TensorFlow model\n",
    "tf.reset_default_graph()\n",
    "max_seq_length = 10\n",
    "tf_model = StaticSentenceEncoder(vocab_size, embedding_dim, hidden_dim, max_seq_length)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Process sentences of different lengths\n",
    "    short_sentence = [[1, 2, 3, 4, 5, 0, 0, 0, 0, 0]]  # Padded to max_seq_length\n",
    "    long_sentence = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "    \n",
    "    short_encoding = sess.run(tf_model.state, \n",
    "                              feed_dict={tf_model.inputs: short_sentence, tf_model.seq_lengths: [5]})\n",
    "    long_encoding = sess.run(tf_model.state, \n",
    "                             feed_dict={tf_model.inputs: long_sentence, tf_model.seq_lengths: [10]})\n",
    "    \n",
    "    print(\"TensorFlow (Static) - Short sentence encoding shape:\", short_encoding.shape)\n",
    "    print(\"TensorFlow (Static) - Long sentence encoding shape:\", long_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let me explain the key differences and advantages of each approach:\n",
    "\n",
    "1. PyTorch (Dynamic Graph):\n",
    "   - The model definition is more straightforward and intuitive.\n",
    "   - We can process sentences of different lengths without modification to the model or padding.\n",
    "   - The computational graph is built on-the-fly during the forward pass.\n",
    "   - It's easier to debug as you can use standard Python debugging tools.\n",
    "   - Changes to the model structure can be made easily without recompiling.\n",
    "\n",
    "2. TensorFlow 1.x (Static Graph):\n",
    "   - We need to define placeholders for inputs and sequence lengths.\n",
    "   - The maximum sequence length must be defined beforehand.\n",
    "   - Shorter sentences need to be padded to the maximum length.\n",
    "   - The graph is defined first, then compiled and executed.\n",
    "   - It's more efficient for repeated executions once compiled.\n",
    "   - Debugging can be more challenging as the execution happens in a separate session.\n",
    "\n",
    "The key advantage of the dynamic graph (PyTorch) in this NLP context is its flexibility with variable-length inputs. In the static graph approach, we had to set a maximum sequence length and pad shorter sentences, which can be inefficient for datasets with widely varying sentence lengths.\n",
    "\n",
    "This example demonstrates why dynamic graphs are particularly useful in NLP tasks, where input structures can vary significantly between samples. They allow for more natural handling of variable-length sequences and enable easier experimentation with different model architectures during research and development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! I'll demonstrate how changes to the model structure can be made easily in PyTorch (dynamic graph) compared to TensorFlow 1.x (static graph). We'll modify our sentence encoder to include an additional layer and change the RNN type.\n",
    "\n",
    "Let's start with the PyTorch example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation (Dynamic Graph)\n",
    "class DynamicSentenceEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embedded = self.embedding(sentence)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "# Example usage of PyTorch model\n",
    "vocab_size, embedding_dim, hidden_dim = 1000, 50, 100\n",
    "pytorch_model = DynamicSentenceEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Process a sentence\n",
    "sentence = torch.LongTensor([[1, 2, 3, 4, 5]])\n",
    "encoding = pytorch_model(sentence)\n",
    "print(\"PyTorch (Dynamic) - Original encoding shape:\", encoding.shape)\n",
    "\n",
    "# Modify the model structure\n",
    "pytorch_model.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "pytorch_model.fc = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    ")\n",
    "\n",
    "# Process the same sentence with the modified model\n",
    "new_encoding = pytorch_model(sentence)\n",
    "print(\"PyTorch (Dynamic) - Modified encoding shape:\", new_encoding.shape)\n",
    "\n",
    "# TensorFlow 1.x implementation (Static Graph)\n",
    "class StaticSentenceEncoder:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, max_seq_length):\n",
    "        self.inputs = tf.placeholder(tf.int32, shape=[None, max_seq_length])\n",
    "        self.seq_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, embedding_dim])\n",
    "        embedded = tf.nn.embedding_lookup(embedding, self.inputs)\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.GRUCell(hidden_dim)\n",
    "        _, state = tf.nn.dynamic_rnn(cell, embedded, sequence_length=self.seq_lengths, dtype=tf.float32)\n",
    "        \n",
    "        self.output = tf.layers.dense(state, hidden_dim // 2)\n",
    "\n",
    "# Example usage of TensorFlow model\n",
    "tf.reset_default_graph()\n",
    "max_seq_length = 10\n",
    "tf_model = StaticSentenceEncoder(vocab_size, embedding_dim, hidden_dim, max_seq_length)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Process a sentence\n",
    "    sentence = [[1, 2, 3, 4, 5, 0, 0, 0, 0, 0]]  # Padded to max_seq_length\n",
    "    encoding = sess.run(tf_model.output, \n",
    "                        feed_dict={tf_model.inputs: sentence, tf_model.seq_lengths: [5]})\n",
    "    print(\"TensorFlow (Static) - Original encoding shape:\", encoding.shape)\n",
    "\n",
    "# To modify the TensorFlow model, we need to redefine the entire graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class ModifiedStaticSentenceEncoder:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, max_seq_length):\n",
    "        self.inputs = tf.placeholder(tf.int32, shape=[None, max_seq_length])\n",
    "        self.seq_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, embedding_dim])\n",
    "        embedded = tf.nn.embedding_lookup(embedding, self.inputs)\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.LSTMCell(hidden_dim)\n",
    "        _, state = tf.nn.dynamic_rnn(cell, embedded, sequence_length=self.seq_lengths, dtype=tf.float32)\n",
    "        \n",
    "        hidden = tf.layers.dense(state.h, hidden_dim // 2, activation=tf.nn.relu)\n",
    "        self.output = tf.layers.dense(hidden, hidden_dim // 4)\n",
    "\n",
    "# Use the modified TensorFlow model\n",
    "tf_model = ModifiedStaticSentenceEncoder(vocab_size, embedding_dim, hidden_dim, max_seq_length)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Process the same sentence with the modified model\n",
    "    new_encoding = sess.run(tf_model.output, \n",
    "                            feed_dict={tf_model.inputs: sentence, tf_model.seq_lengths: [5]})\n",
    "    print(\"TensorFlow (Static) - Modified encoding shape:\", new_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's break down the key differences in modifying the model structure:\n",
    "\n",
    "1. PyTorch (Dynamic Graph):\n",
    "   - We can directly modify the existing model by changing its attributes:\n",
    "     ```python\n",
    "     pytorch_model.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "     pytorch_model.fc = nn.Sequential(\n",
    "         nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "     )\n",
    "     ```\n",
    "   - These changes take effect immediately, and we can use the modified model right away.\n",
    "   - No recompilation is needed; the new graph is built on the next forward pass.\n",
    "   - We can even modify the model structure based on runtime conditions if needed.\n",
    "\n",
    "2. TensorFlow 1.x (Static Graph):\n",
    "   - To change the model structure, we need to redefine the entire graph:\n",
    "     ```python\n",
    "     tf.reset_default_graph()\n",
    "     class ModifiedStaticSentenceEncoder:\n",
    "         # ... (entire new class definition)\n",
    "     ```\n",
    "   - We create a new instance of the modified model.\n",
    "   - The session needs to be reinitialized with the new graph.\n",
    "   - Any saved state from the previous model is lost unless explicitly handled.\n",
    "   - Changes cannot be made dynamically during runtime; the entire graph must be predefined.\n",
    "\n",
    "The PyTorch approach allows for more flexible and incremental changes to the model structure. This is particularly useful in research settings where you might want to experiment with different architectures quickly. You can even write code that modifies the model based on certain conditions or input characteristics.\n",
    "\n",
    "In contrast, the TensorFlow 1.x static graph approach requires more upfront planning. Once the graph is defined and compiled, it's more difficult to make structural changes. This can be advantageous for production environments where the model structure is fixed, as it allows for optimizations that can improve performance.\n",
    "\n",
    "It's worth noting that TensorFlow 2.x has adopted an eager execution mode that is more similar to PyTorch's dynamic graph approach, allowing for more flexible model modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
